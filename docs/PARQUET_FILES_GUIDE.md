# –ß—Ç–æ —Ç–∞–∫–æ–µ Parquet —Ñ–∞–π–ª—ã - –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ

**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è**: 20 —è–Ω–≤–∞—Ä—è 2026
**–ü—Ä–æ–µ–∫—Ç**: –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫

---

## –í–≤–µ–¥–µ–Ω–∏–µ

**Apache Parquet** - —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (BigData), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏.

---

## –û—Å–Ω–æ–≤–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ Parquet

### 1. –ö–æ–ª–æ–Ω–æ—á–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö

–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –æ–±—ã—á–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü (CSV, Excel), –≥–¥–µ –¥–∞–Ω–Ω—ã–µ —Ö—Ä–∞–Ω—è—Ç—Å—è –ø–æ—Å—Ç—Ä–æ—á–Ω–æ, Parquet —Ö—Ä–∞–Ω–∏—Ç –¥–∞–Ω–Ω—ã–µ **–ø–æ –∫–æ–ª–æ–Ω–∫–∞–º**:

#### –û–±—ã—á–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ (CSV) - –ø–æ—Å—Ç—Ä–æ—á–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ:
```
ID, –ò–º—è, –¶–µ–Ω–∞, –î–∞—Ç–∞
1, –¢–æ–≤–∞—Ä1, 100, 2025-01-01
2, –¢–æ–≤–∞—Ä2, 200, 2025-01-02
3, –¢–æ–≤–∞—Ä3, 150, 2025-01-03
```

#### Parquet - –∫–æ–ª–æ–Ω–æ—á–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ:
```
–ö–æ–ª–æ–Ω–∫–∞ ID:    [1, 2, 3]
–ö–æ–ª–æ–Ω–∫–∞ –ò–º—è:   ["–¢–æ–≤–∞—Ä1", "–¢–æ–≤–∞—Ä2", "–¢–æ–≤–∞—Ä3"]
–ö–æ–ª–æ–Ω–∫–∞ –¶–µ–Ω–∞:  [100, 200, 150]
–ö–æ–ª–æ–Ω–∫–∞ –î–∞—Ç–∞:  [2025-01-01, 2025-01-02, 2025-01-03]
```

### 2. –ú–æ—â–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö

- **–°–∂–∞—Ç–∏–µ –≤ 10-20 —Ä–∞–∑** –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å CSV
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã: Snappy, Gzip, LZO, Zstd
- –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫–æ–ª–æ–Ω–∫–µ —Å–∂–∏–º–∞—é—Ç—Å—è –æ—á–µ–Ω—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å–ª–æ–≤–∞—Ä–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∑–Ω–∞—á–µ–Ω–∏–π

### 3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö:**
- –ß—Ç–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ (–Ω–µ –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∂–∞—Ç—å –≤—Å—é —Ç–∞–±–ª–∏—Ü—É)
- –ü—Ä–µ–¥–∏–∫–∞—Ç pushdown - —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ñ–∞–π–ª–∞
- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º (–º–∏–Ω/–º–∞–∫—Å –∑–Ω–∞—á–µ–Ω–∏—è)
- –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø–∞—Ä—Ç–∏—Ü–∏–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏

### 4. –ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è

Parquet - —Å—Ç–∞–Ω–¥–∞—Ä—Ç –≤ —ç–∫–æ—Å–∏—Å—Ç–µ–º–µ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö:
- **Apache Spark** - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- **Apache Hadoop** - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
- **Databricks** - –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
- **AWS Athena** - –æ–±–ª–∞—á–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ SQL
- **Google BigQuery** - –æ–±–ª–∞—á–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–∞–Ω–Ω—ã—Ö
- **Pandas** - –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –≤ Python
- **Dask** - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è

---

## –ü–æ—á–µ–º—É —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤ 17-18 —Ä–∞–∑ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ CSV?

### –ú–µ—Ö–∞–Ω–∏–∑–º —Å–∂–∞—Ç–∏—è –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ

–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ —Ç–∞–±–ª–∏—Ü—É —Å –º–∏–ª–ª–∏–æ–Ω–æ–º —Å—Ç—Ä–æ–∫ –ø—Ä–æ–¥–∞–∂:
- –ö–æ–ª–æ–Ω–∫–∞ "–ì–æ—Ä–æ–¥": 80% —Å—Ç—Ä–æ–∫ —Å–æ–¥–µ—Ä–∂–∞—Ç "–ú–æ—Å–∫–≤–∞"
- –ö–æ–ª–æ–Ω–∫–∞ "–°—Ç–∞—Ç—É—Å": —Ç–æ–ª—å–∫–æ 3 –∑–Ω–∞—á–µ–Ω–∏—è ("–ù–æ–≤—ã–π", "–í –æ–±—Ä–∞–±–æ—Ç–∫–µ", "–ó–∞–≤–µ—Ä—à–µ–Ω")

#### –í —Ñ–æ—Ä–º–∞—Ç–µ Parquet (—Å–∂–∞—Ç—ã–π):
```
–ö–æ–ª–æ–Ω–∫–∞ "–ì–æ—Ä–æ–¥":
  –°–ª–æ–≤–∞—Ä—å: ["–ú–æ—Å–∫–≤–∞" ‚Üí ID:1, "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥" ‚Üí ID:2, ...]
  –î–∞–Ω–Ω—ã–µ: [1,1,1,1,1,1,2,1,1,1...] (—Ç–æ–ª—å–∫–æ ID)
  –†–∞–∑–º–µ—Ä: ~10 –±–∞–π—Ç –¥–ª—è –∑–Ω–∞—á–µ–Ω–∏—è + 1 –±–∞–π—Ç √ó 1,000,000 = ~1 –ú–ë
```

#### –í —Ñ–æ—Ä–º–∞—Ç–µ CSV (–Ω–µ—Å–∂–∞—Ç—ã–π —Ç–µ–∫—Å—Ç):
```
–ú–æ—Å–∫–≤–∞,–ú–æ—Å–∫–≤–∞,–ú–æ—Å–∫–≤–∞,–ú–æ—Å–∫–≤–∞,–ú–æ—Å–∫–≤–∞,–ú–æ—Å–∫–≤–∞,–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥,–ú–æ—Å–∫–≤–∞...
–†–∞–∑–º–µ—Ä: "–ú–æ—Å–∫–≤–∞" √ó 800,000 —Ä–∞–∑ = 800,000 √ó 6 –±–∞–π—Ç = 4.8 –ú–ë
        + –¥—Ä—É–≥–∏–µ –≥–æ—Ä–æ–¥–∞
        = ~6-8 –ú–ë —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–¥–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏!
```

### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã —É–≤–µ–ª–∏—á–µ–Ω–∏—è:

1. **–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:**
   - Parquet: —á–∏—Å–ª–∞ —Ö—Ä–∞–Ω—è—Ç—Å—è –∫–∞–∫ –±–∏–Ω–∞—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (4-8 –±–∞–π—Ç)
   - CSV: —á–∏—Å–ª–∞ –∫–∞–∫ —Ç–µ–∫—Å—Ç ("1000000" = 7 –±–∞–π—Ç –≤–º–µ—Å—Ç–æ 4)

2. **–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:**
   - Parquet: —Å–∂–∞—Ç—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –∫–æ–Ω—Ü–µ —Ñ–∞–π–ª–∞
   - CSV: –∑–∞–ø—è—Ç—ã–µ, –∫–∞–≤—ã—á–∫–∏, –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫

3. **Null –∑–Ω–∞—á–µ–Ω–∏—è:**
   - Parquet: –±–∏—Ç–æ–≤–∞—è –º–∞—Å–∫–∞ (1 –±–∏—Ç –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏–µ)
   - CSV: –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏–ª–∏ "NULL" (4 –±–∞–π—Ç–∞)

---

## –í–∞—à–∏ Parquet —Ñ–∞–π–ª—ã –≤ –ø—Ä–æ–µ–∫—Ç–µ

### –†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ:
```
localdata/markdown_files/
‚îî‚îÄ‚îÄ –†–û–õ–õ–ê–£–¢/
    ‚îî‚îÄ‚îÄ –ú–µ–≥–∞—Ñ–æ–Ω/
        ‚îî‚îÄ‚îÄ –†–æ–ª–ª-–∞—É—Ç/
            ‚îî‚îÄ‚îÄ –î–∞–Ω–Ω—ã–µ –æ—Ç –º–µ–≥–∞—Ñ–æ–Ω/
                ‚îî‚îÄ‚îÄ bigdata/
                    ‚îú‚îÄ‚îÄ stock/          # –û—Å—Ç–∞—Ç–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤
                    ‚îú‚îÄ‚îÄ stock2/
                    ‚îú‚îÄ‚îÄ transactions/   # –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –ø—Ä–æ–¥–∞–∂
                    ‚îî‚îÄ‚îÄ transactions2/
```

### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤–∞—à–∏–º —Ñ–∞–π–ª–∞–º:

#### 1. –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (100 —Ñ–∞–π–ª–æ–≤)
- **–ò–º–µ–Ω–∞**: `part-*-be3eeb7b-674a-4738-8013-58ed82486f5f-c000.snappy.parquet`
- **–ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä**: ~5.6 –ì–ë (—Å–∂–∞—Ç—ã–µ Parquet)
- **–ü–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ CSV**: ~35 –ì–ë
- **–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç**: **6.25x**
- **–°–æ–¥–µ—Ä–∂–∏–º–æ–µ**: –î–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥–∞–∂–∞—Ö, —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è—Ö –ú–µ–≥–∞—Ñ–æ–Ω

#### 2. –û—Å—Ç–∞—Ç–∫–∏ (30 —Ñ–∞–π–ª–æ–≤)
- **–ò–º–µ–Ω–∞**: `part-*-9affeaf0-e8ea-4003-9154-75efc79ad660-c000.snappy.parquet`
- **–ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä**: ~1.8 –ì–ë (—Å–∂–∞—Ç—ã–µ Parquet)
- **–ü–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ CSV**: ~20 –ì–ë
- **–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç**: **11.1x**
- **–°–æ–¥–µ—Ä–∂–∏–º–æ–µ**: –î–∞–Ω–Ω—ã–µ –æ–± –æ—Å—Ç–∞—Ç–∫–∞—Ö —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Å–∫–ª–∞–¥–∞—Ö

#### 3. –î—Ä—É–≥–∏–µ —Ñ–∞–π–ª—ã (21 —Ñ–∞–π–ª)
- **–†–∞–∑–º–µ—Ä**: ~1.6 –ì–ë ‚Üí ~8 –ì–ë
- **–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç**: **5x**

### –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:

| –ú–µ—Ç—Ä–∏–∫–∞ | –ó–Ω–∞—á–µ–Ω–∏–µ |
|---------|----------|
| **–í—Å–µ–≥–æ Parquet —Ñ–∞–π–ª–æ–≤** | 151 |
| **–ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä (Parquet)** | 9.0 –ì–ë |
| **–ü–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ (CSV)** | 63.0 –ì–ë |
| **–£–≤–µ–ª–∏—á–µ–Ω–∏–µ** | **54 –ì–ë** |
| **–°—Ä–µ–¥–Ω–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç** | **7x** |

### –¢–û–ü-10 —Ñ–∞–π–ª–æ–≤ –ø–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—É —É–≤–µ–ª–∏—á–µ–Ω–∏—è:

| ‚Ññ | –§–∞–π–ª | –ò—Å—Ö–æ–¥–Ω—ã–π | –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π | –ö–æ—ç—Ñ—Ñ. |
|---|------|----------|------------------|--------|
| 1 | part-00002-be3eeb7b-674a...snappy.parquet | 55.7 –ú–ë | 1.0 –ì–ë | **18.3x** |
| 2 | part-00008-be3eeb7b-674a...snappy.parquet | 55.7 –ú–ë | 1.0 –ì–ë | **18.3x** |
| 3 | part-00004-be3eeb7b-674a...snappy.parquet | 55.8 –ú–ë | 1.0 –ì–ë | **18.3x** |
| 4 | part-00005-be3eeb7b-674a...snappy.parquet | 55.9 –ú–ë | 1.0 –ì–ë | **18.3x** |
| 5 | part-00007-be3eeb7b-674a...snappy.parquet | 55.9 –ú–ë | 1.0 –ì–ë | **18.3x** |
| 6 | part-00009-be3eeb7b-674a...snappy.parquet | 55.9 –ú–ë | 1.0 –ì–ë | **18.2x** |
| 7 | part-00003-be3eeb7b-674a...snappy.parquet | 56.0 –ú–ë | 1.0 –ì–ë | **18.2x** |
| 8 | part-00001-be3eeb7b-674a...snappy.parquet | 56.0 –ú–ë | 1.0 –ì–ë | **18.2x** |
| 9 | part-00000-be3eeb7b-674a...snappy.parquet | 56.0 –ú–ë | 1.0 –ì–ë | **18.2x** |
| 10 | part-00014-9affeaf0-e8ea...snappy.parquet | 59.5 –ú–ë | 1.0 –ì–ë | **17.3x** |

**–í—ã–≤–æ–¥**: –ö–∞–∂–¥—ã–π —Ñ–∞–π–ª ~56-60 –ú–ë –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ ~1 –ì–ë CSV —Ñ–∞–π–ª–∞!

---

## –ö–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å —Å Parquet —Ñ–∞–π–ª–∞–º–∏

### ‚ùå –ù–ï –¥–µ–ª–∞–π—Ç–µ —Ç–∞–∫ (—Ç–µ–∫—É—â–∞—è —Å–∏—Ç—É–∞—Ü–∏—è):

```python
# –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è Parquet ‚Üí CSV –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞
import pandas as pd
df = pd.read_parquet('file.parquet')
df.to_csv('file.csv')  # 56 –ú–ë ‚Üí 1 –ì–ë!
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
- –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤ 18 —Ä–∞–∑
- –ü–æ—Ç–µ—Ä—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
- –ú–µ–¥–ª–µ–Ω–Ω–æ–µ —á—Ç–µ–Ω–∏–µ CSV
- –†–∞—Å—Ö–æ–¥ –¥–∏—Å–∫–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞

### ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã:

#### 1. Python + Pandas (–¥–ª—è —Ñ–∞–π–ª–æ–≤ –¥–æ 10 –ì–ë)

```python
import pandas as pd

# –ß–∏—Ç–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é –∏–∑ Parquet
df = pd.read_parquet('file.parquet')

# –†–∞–±–æ—Ç–∞–µ–º —Å –¥–∞–Ω–Ω—ã–º–∏
print(df.head())                         # –ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
print(df.describe())                     # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
print(df['—Ü–µ–Ω–∞'].mean())                 # –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞

# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
filtered = df[df['—Ü–µ–Ω–∞'] > 100]

# –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞
grouped = df.groupby('–≥–æ—Ä–æ–¥')['–ø—Ä–æ–¥–∞–∂–∏'].sum()

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ Parquet
filtered.to_parquet('result.parquet')
```

#### 2. Apache Spark (–¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö > 10 –ì–ë)

```python
from pyspark.sql import SparkSession

# –°–æ–∑–¥–∞–µ–º Spark —Å–µ—Å—Å–∏—é
spark = SparkSession.builder.appName("Analysis").getOrCreate()

# –ß–∏—Ç–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏
df = spark.read.parquet('bigdata/stock/')

# SQL –∑–∞–ø—Ä–æ—Å—ã
df.createOrReplaceTempView("stock")
result = spark.sql("""
    SELECT –≥–æ—Ä–æ–¥, SUM(–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ) as total
    FROM stock
    WHERE –¥–∞—Ç–∞ >= '2025-01-01'
    GROUP BY –≥–æ—Ä–æ–¥
    ORDER BY total DESC
""")

result.show()

# –ò–ª–∏ —á–µ—Ä–µ–∑ DataFrame API
df.filter(df.date >= '2025-01-01') \
  .groupBy('–≥–æ—Ä–æ–¥') \
  .sum('–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ') \
  .orderBy('sum(–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ)', ascending=False) \
  .show()
```

#### 3. DuckDB (SQL –¥–ª—è —Ñ–∞–π–ª–æ–≤)

```python
import duckdb

# SQL –∑–∞–ø—Ä–æ—Å—ã –ø—Ä—è–º–æ –∫ Parquet —Ñ–∞–π–ª–∞–º
con = duckdb.connect()

# –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
result = con.execute("""
    SELECT *
    FROM 'file.parquet'
    WHERE —Ü–µ–Ω–∞ > 100
    LIMIT 10
""").fetchdf()

# –ß–∏—Ç–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏
result = con.execute("""
    SELECT –≥–æ—Ä–æ–¥, COUNT(*) as count, AVG(—Ü–µ–Ω–∞) as avg_price
    FROM 'bigdata/stock/*.parquet'
    GROUP BY –≥–æ—Ä–æ–¥
    ORDER BY count DESC
""").fetchdf()

print(result)
```

#### 4. PyArrow (–Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –¥–æ—Å—Ç—É–ø)

```python
import pyarrow.parquet as pq

# –ß–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
parquet_file = pq.ParquetFile('file.parquet')
print(parquet_file.metadata)
print(parquet_file.schema)

# –ß–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
table = pq.read_table('file.parquet', columns=['–≥–æ—Ä–æ–¥', '—Ü–µ–Ω–∞'])
df = table.to_pandas()

# –ß–∏—Ç–∞–µ–º –ø–æ —á–∞—Å—Ç—è–º (–¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤)
for batch in parquet_file.iter_batches(batch_size=10000):
    df = batch.to_pandas()
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–π –ø–æ—Ä—Ü–∏–∏
    print(df.head())
```

#### 5. Polars (–±—ã—Å—Ç—Ä–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ Pandas)

```python
import polars as pl

# –ë—ã—Å—Ç—Ä–µ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ, —á–µ–º Pandas
df = pl.read_parquet('file.parquet')

# –õ–µ–Ω–∏–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è (lazy evaluation)
result = (
    pl.scan_parquet('bigdata/stock/*.parquet')
    .filter(pl.col('—Ü–µ–Ω–∞') > 100)
    .groupby('–≥–æ—Ä–æ–¥')
    .agg(pl.col('–ø—Ä–æ–¥–∞–∂–∏').sum())
    .sort('–ø—Ä–æ–¥–∞–∂–∏', descending=True)
    .collect()  # –í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å
)

print(result)
```

---

## –ü—Ä–∏–º–µ—Ä—ã —Ä–∞–±–æ—Ç—ã —Å –≤–∞—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏

### –ü—Ä–∏–º–µ—Ä 1: –ü—Ä–æ—Å–º–æ—Ç—Ä —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ

```python
import pandas as pd
from pathlib import Path

# –ü—É—Ç—å –∫ –≤–∞—à–∏–º —Ñ–∞–π–ª–∞–º
base_path = Path('localdata/markdown_files/–†–û–õ–õ–ê–£–¢/–ú–µ–≥–∞—Ñ–æ–Ω/–†–æ–ª–ª-–∞—É—Ç/–î–∞–Ω–Ω—ã–µ –æ—Ç –º–µ–≥–∞—Ñ–æ–Ω/bigdata')

# –ß–∏—Ç–∞–µ–º –æ–¥–∏–Ω —Ñ–∞–π–ª —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
transactions_file = base_path / 'transactions2/part-00000-be3eeb7b-674a-4738-8013-58ed82486f5f-c000.snappy.parquet'
df = pd.read_parquet(transactions_file)

print(f"–†–∞–∑–º–µ—Ä: {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
print(f"\n–ö–æ–ª–æ–Ω–∫–∏: {df.columns.tolist()}")
print(f"\n–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫:")
print(df.head())
print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
print(df.describe())
```

### –ü—Ä–∏–º–µ—Ä 2: –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π

```python
import pandas as pd
from pathlib import Path

base_path = Path('localdata/markdown_files/–†–û–õ–õ–ê–£–¢/–ú–µ–≥–∞—Ñ–æ–Ω/–†–æ–ª–ª-–∞—É—Ç/–î–∞–Ω–Ω—ã–µ –æ—Ç –º–µ–≥–∞—Ñ–æ–Ω/bigdata')

# –ß–∏—Ç–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
all_files = list((base_path / 'transactions2').glob('*.parquet'))
print(f"–ù–∞–π–¥–µ–Ω–æ {len(all_files)} —Ñ–∞–π–ª–æ–≤")

# –ß–∏—Ç–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º
dfs = []
for file in all_files[:5]:  # –ü–µ—Ä–≤—ã–µ 5 –¥–ª—è —Ç–µ—Å—Ç–∞
    df = pd.read_parquet(file)
    dfs.append(df)

combined = pd.concat(dfs, ignore_index=True)
print(f"\n–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(combined):,}")

# –ê–Ω–∞–ª–∏–∑
if 'date' in combined.columns or '–¥–∞—Ç–∞' in combined.columns:
    date_col = 'date' if 'date' in combined.columns else '–¥–∞—Ç–∞'
    print(f"\n–ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö: {combined[date_col].min()} - {combined[date_col].max()}")

if 'amount' in combined.columns or '—Å—É–º–º–∞' in combined.columns:
    amount_col = 'amount' if 'amount' in combined.columns else '—Å—É–º–º–∞'
    print(f"–û–±—â–∞—è —Å—É–º–º–∞: {combined[amount_col].sum():,.2f}")
    print(f"–°—Ä–µ–¥–Ω–∏–π —á–µ–∫: {combined[amount_col].mean():,.2f}")
```

### –ü—Ä–∏–º–µ—Ä 3: –≠–∫—Å–ø–æ—Ä—Ç —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ CSV

```python
import pandas as pd

# –ß–∏—Ç–∞–µ–º Parquet
df = pd.read_parquet('file.parquet')

# –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ–µ
filtered = df[df['–¥–∞—Ç–∞'] >= '2025-01-01'].head(1000)

# –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 1000 —Å—Ç—Ä–æ–∫
filtered.to_csv('preview.csv', index=False)
print(f"–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(filtered)} —Å—Ç—Ä–æ–∫")
```

### –ü—Ä–∏–º–µ—Ä 4: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –±–µ–∑ –ø–æ–ª–Ω–æ–π –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏

```python
import pandas as pd
from pathlib import Path

def create_parquet_summary(parquet_file):
    """–°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –ø–æ Parquet —Ñ–∞–π–ª—É"""
    df = pd.read_parquet(parquet_file)

    summary = {
        'file': parquet_file.name,
        'rows': len(df),
        'columns': len(df.columns),
        'column_names': df.columns.tolist(),
        'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,
        'date_range': None,
        'sample_data': df.head(5).to_dict()
    }

    # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –¥–∞—Ç–∞–º–∏
    for col in df.columns:
        if 'date' in col.lower() or '–¥–∞—Ç–∞' in col.lower():
            summary['date_range'] = f"{df[col].min()} - {df[col].max()}"
            break

    return summary

# –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–∫–∏ –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
base_path = Path('localdata/markdown_files/–†–û–õ–õ–ê–£–¢/–ú–µ–≥–∞—Ñ–æ–Ω/–†–æ–ª–ª-–∞—É—Ç/–î–∞–Ω–Ω—ã–µ –æ—Ç –º–µ–≥–∞—Ñ–æ–Ω/bigdata')
summaries = []

for parquet_file in base_path.rglob('*.parquet'):
    try:
        summary = create_parquet_summary(parquet_file)
        summaries.append(summary)
        print(f"‚úì {parquet_file.name}: {summary['rows']:,} —Å—Ç—Ä–æ–∫")
    except Exception as e:
        print(f"‚úó {parquet_file.name}: –û—à–∏–±–∫–∞ - {e}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–∫–∏ –≤ JSON
import json
with open('parquet_summaries.json', 'w', encoding='utf-8') as f:
    json.dump(summaries, f, ensure_ascii=False, indent=2, default=str)

print(f"\n–°–æ–∑–¥–∞–Ω–æ {len(summaries)} —Å–≤–æ–¥–æ–∫")
```

---

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫

### –ë–∞–∑–æ–≤–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ (Pandas + PyArrow):
```bash
pip install pandas pyarrow
```

### –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ (—Å–æ –≤—Å–µ–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏):
```bash
pip install pandas pyarrow polars duckdb fastparquet
```

### –î–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (Spark):
```bash
pip install pyspark
```

---

## –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–∞–Ω–Ω—ã—Ö

| –§–æ—Ä–º–∞—Ç | –†–∞–∑–º–µ—Ä | –°–∫–æ—Ä–æ—Å—Ç—å —á—Ç–µ–Ω–∏—è | –°–∂–∞—Ç–∏–µ | –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö | –î–ª—è —á–µ–≥–æ |
|--------|--------|-----------------|--------|-------------|----------|
| **Parquet** | ‚≠ê 1x | ‚ö°‚ö°‚ö° –ë—ã—Å—Ç—Ä–æ | ‚úÖ –û—Ç–ª–∏—á–Ω–æ | ‚úÖ –°—Ç—Ä–æ–≥–∏–µ | BigData, –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ |
| **CSV** | ‚ùå 10-20x | üêå –ú–µ–¥–ª–µ–Ω–Ω–æ | ‚ùå –ù–µ—Ç | ‚ùå –¢–µ–∫—Å—Ç | –ü—Ä–æ—Å—Ç–æ—Ç–∞, Excel |
| **Excel (xlsx)** | ‚ö†Ô∏è 5-7x | üêå –ú–µ–¥–ª–µ–Ω–Ω–æ | ‚úÖ –°—Ä–µ–¥–Ω–µ | ‚ö†Ô∏è –ù–µ—Ç–æ—á–Ω—ã–µ | –û—Ñ–∏—Å–Ω–∞—è —Ä–∞–±–æ—Ç–∞ |
| **JSON** | ‚ùå 10-15x | üêå –ú–µ–¥–ª–µ–Ω–Ω–æ | ‚ùå –ù–µ—Ç | ‚ö†Ô∏è –°–ª–∞–±—ã–µ | API, –≤–µ–± |
| **Avro** | ‚≠ê 1-2x | ‚ö°‚ö° –ë—ã—Å—Ç—Ä–æ | ‚úÖ –•–æ—Ä–æ—à–æ | ‚úÖ –°—Ç—Ä–æ–≥–∏–µ | –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ |
| **ORC** | ‚≠ê 0.8-1x | ‚ö°‚ö°‚ö° –ë—ã—Å—Ç—Ä–æ | ‚úÖ –û—Ç–ª–∏—á–Ω–æ | ‚úÖ –°—Ç—Ä–æ–≥–∏–µ | Hadoop —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞ |

### –î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ (1 –º–ª–Ω —Å—Ç—Ä–æ–∫):

| –û–ø–µ—Ä–∞—Ü–∏—è | Parquet | CSV | –†–∞–∑–Ω–∏—Ü–∞ |
|----------|---------|-----|---------|
| **–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞** | 50 –ú–ë | 900 –ú–ë | **18x** |
| **–ß—Ç–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö** | 0.5 —Å–µ–∫ | 8 —Å–µ–∫ | **16x** |
| **–ß—Ç–µ–Ω–∏–µ 1 –∫–æ–ª–æ–Ω–∫–∏** | 0.1 —Å–µ–∫ | 8 —Å–µ–∫ | **80x** |
| **–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è** | 0.3 —Å–µ–∫ | 8 —Å–µ–∫ | **27x** |
| **–ü–∞–º—è—Ç—å** | 200 –ú–ë | 2 –ì–ë | **10x** |

---

## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –≤–∞—à–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞

### üéØ –ß—Ç–æ –¥–µ–ª–∞—Ç—å —Å Parquet —Ñ–∞–π–ª–∞–º–∏

#### –í–∞—Ä–∏–∞–Ω—Ç 1: –ù–µ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å (–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø)

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –≠–∫–æ–Ω–æ–º–∏—è **54 –ì–ë** –¥–∏—Å–∫–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
- ‚úÖ –ë—ã—Å—Ç—Ä—ã–π –¥–æ—Å—Ç—É–ø –∫ –¥–∞–Ω–Ω—ã–º —á–µ—Ä–µ–∑ pandas
- ‚úÖ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ
- ‚úÖ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å SQL –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ DuckDB

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
# –ò–∑–º–µ–Ω–∏—Ç—å –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ src/config.py
SKIP_CONVERSION_FORMATS = ['.parquet', '.snappy.parquet']

# –ò–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –≤ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–µ
def should_convert(file_path):
    if file_path.suffix == '.parquet':
        return False
    # ... –æ—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞
```

#### –í–∞—Ä–∏–∞–Ω—Ç 2: –°–æ–∑–¥–∞–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ + —Å—ç–º–ø–ª

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –≠–∫–æ–Ω–æ–º–∏—è –º–µ—Å—Ç–∞
- ‚úÖ –ë—ã—Å—Ç—Ä—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
- ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
import pandas as pd
from pathlib import Path

def create_parquet_preview(parquet_path, output_path):
    """–°–æ–∑–¥–∞–µ—Ç Markdown —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º Parquet —Ñ–∞–π–ª–∞"""
    df = pd.read_parquet(parquet_path)

    # –°–æ–∑–¥–∞–µ–º Markdown
    md_content = f"""# {parquet_path.name}

## –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ

- **–°—Ç—Ä–æ–∫**: {len(df):,}
- **–ö–æ–ª–æ–Ω–æ–∫**: {len(df.columns)}
- **–†–∞–∑–º–µ—Ä –≤ –ø–∞–º—è—Ç–∏**: {df.memory_usage(deep=True).sum() / 1024**2:.1f} –ú–ë

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö

| –ö–æ–ª–æ–Ω–∫–∞ | –¢–∏–ø | Null | –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö |
|---------|-----|------|------------|
"""

    for col in df.columns:
        dtype = df[col].dtype
        nulls = df[col].isnull().sum()
        unique = df[col].nunique()
        md_content += f"| {col} | {dtype} | {nulls:,} | {unique:,} |\n"

    md_content += f"""

## –ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫

{df.head(10).to_markdown()}

## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

{df.describe().to_markdown()}

---

**–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª**: `{parquet_path.relative_to('localdata/markdown_files')}`
**–§–æ—Ä–º–∞—Ç**: Apache Parquet
**–î–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞**: `pd.read_parquet('{parquet_path}')`
"""

    output_path.write_text(md_content, encoding='utf-8')
    return len(md_content)
```

#### –í–∞—Ä–∏–∞–Ω—Ç 3: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–µ–±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã

```python
def convert_if_small(parquet_path, max_size_mb=100):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã –º–µ–Ω—å—à–µ max_size_mb"""
    size_mb = parquet_path.stat().st_size / 1024**2

    if size_mb > max_size_mb:
        print(f"–ü—Ä–æ–ø—É—Å–∫ {parquet_path.name} ({size_mb:.1f} –ú–ë > {max_size_mb} –ú–ë)")
        return None

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –º–∞–ª–µ–Ω—å–∫–∏–π —Ñ–∞–π–ª
    df = pd.read_parquet(parquet_path)
    csv_path = parquet_path.with_suffix('.csv')
    df.to_csv(csv_path, index=False)
    return csv_path
```

### üìä –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á–µ—Ç–æ–≤

–í–º–µ—Å—Ç–æ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–≤–∞–π—Ç–µ –≥–æ—Ç–æ–≤—ã–µ –æ—Ç—á–µ—Ç—ã:

```python
import pandas as pd
from pathlib import Path

def create_analytics_report(base_path):
    """–°–æ–∑–¥–∞–µ—Ç –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç –ø–æ –≤—Å–µ–º Parquet —Ñ–∞–π–ª–∞–º"""

    reports = []

    # –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
    transactions = base_path / 'transactions2'
    for file in transactions.glob('*.parquet'):
        df = pd.read_parquet(file)

        report = {
            'file': file.name,
            'type': 'transactions',
            'rows': len(df),
            'columns': list(df.columns),
            'size_mb': file.stat().st_size / 1024**2,
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –∞–≥—Ä–µ–≥–∞—Ç—ã –µ—Å–ª–∏ –µ—Å—Ç—å –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        if 'amount' in df.columns:
            report['total_amount'] = df['amount'].sum()
            report['avg_amount'] = df['amount'].mean()

        if 'date' in df.columns:
            report['date_range'] = {
                'min': str(df['date'].min()),
                'max': str(df['date'].max())
            }

        reports.append(report)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
    import json
    output = Path('analytics_report.json')
    output.write_text(json.dumps(reports, indent=2, default=str), encoding='utf-8')

    print(f"–°–æ–∑–¥–∞–Ω –æ—Ç—á–µ—Ç: {output}")
    print(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(reports)}")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
base_path = Path('localdata/markdown_files/–†–û–õ–õ–ê–£–¢/–ú–µ–≥–∞—Ñ–æ–Ω/–†–æ–ª–ª-–∞—É—Ç/–î–∞–Ω–Ω—ã–µ –æ—Ç –º–µ–≥–∞—Ñ–æ–Ω/bigdata')
create_analytics_report(base_path)
```

---

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–∫—Ä–∏–ø—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã

### –°–∫—Ä–∏–ø—Ç 1: –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö Parquet —Ñ–∞–π–ª–æ–≤

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `analyze_parquet.py`:

```python
"""
–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö Parquet —Ñ–∞–π–ª–æ–≤ –≤ –ø—Ä–æ–µ–∫—Ç–µ
"""
from pathlib import Path
import pandas as pd
import json


def analyze_parquet_files(base_path="localdata/markdown_files"):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ Parquet —Ñ–∞–π–ª—ã"""
    base = Path(base_path)
    results = []

    for parquet_file in base.rglob("*.parquet"):
        try:
            # –ß–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–±—ã—Å—Ç—Ä–æ)
            import pyarrow.parquet as pq
            pf = pq.ParquetFile(parquet_file)

            info = {
                'path': str(parquet_file.relative_to(base)),
                'size_mb': parquet_file.stat().st_size / 1024**2,
                'rows': pf.metadata.num_rows,
                'columns': len(pf.schema),
                'column_names': [field.name for field in pf.schema],
            }

            results.append(info)
            print(f"‚úì {parquet_file.name}: {info['rows']:,} —Å—Ç—Ä–æ–∫, {info['size_mb']:.1f} –ú–ë")

        except Exception as e:
            print(f"‚úó {parquet_file.name}: {e}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output = Path('parquet_analysis.json')
    output.write_text(json.dumps(results, indent=2, ensure_ascii=False))

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_files = len(results)
    total_size_mb = sum(r['size_mb'] for r in results)
    total_rows = sum(r['rows'] for r in results)

    print(f"\n{'='*60}")
    print(f"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {total_files}")
    print(f"–û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size_mb / 1024:.1f} –ì–ë")
    print(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total_rows:,}")
    print(f"–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {total_size_mb / total_files:.1f} –ú–ë")
    print(f"{'='*60}")

    return results


if __name__ == "__main__":
    analyze_parquet_files()
```

### –°–∫—Ä–∏–ø—Ç 2: –ü–æ–∏—Å–∫ –ø–æ Parquet —Ñ–∞–π–ª–∞–º

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `search_parquet.py`:

```python
"""
–ü–æ–∏—Å–∫ –¥–∞–Ω–Ω—ã—Ö –≤ Parquet —Ñ–∞–π–ª–∞—Ö –±–µ–∑ –ø–æ–ª–Ω–æ–π –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
"""
from pathlib import Path
import pandas as pd


def search_in_parquet(base_path, search_term, column=None):
    """–ò—â–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ Parquet —Ñ–∞–π–ª–∞—Ö"""
    base = Path(base_path)
    results = []

    for parquet_file in base.rglob("*.parquet"):
        try:
            df = pd.read_parquet(parquet_file)

            # –ü–æ–∏—Å–∫ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–æ–ª–æ–Ω–∫–µ –∏–ª–∏ –≤–æ –≤—Å–µ—Ö
            if column and column in df.columns:
                matches = df[df[column].astype(str).str.contains(search_term, case=False, na=False)]
            else:
                # –ü–æ–∏—Å–∫ –≤–æ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö
                mask = pd.Series([False] * len(df))
                for col in df.select_dtypes(include=['object']).columns:
                    mask |= df[col].astype(str).str.contains(search_term, case=False, na=False)
                matches = df[mask]

            if len(matches) > 0:
                results.append({
                    'file': str(parquet_file.relative_to(base)),
                    'matches': len(matches),
                    'sample': matches.head(5).to_dict()
                })
                print(f"‚úì {parquet_file.name}: –Ω–∞–π–¥–µ–Ω–æ {len(matches)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")

        except Exception as e:
            print(f"‚úó {parquet_file.name}: {e}")

    return results


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    base = "localdata/markdown_files/–†–û–õ–õ–ê–£–¢/–ú–µ–≥–∞—Ñ–æ–Ω/–†–æ–ª–ª-–∞—É—Ç/–î–∞–Ω–Ω—ã–µ –æ—Ç –º–µ–≥–∞—Ñ–æ–Ω/bigdata"

    # –ü–æ–∏—Å–∫
    results = search_in_parquet(base, "–ú–æ—Å–∫–≤–∞", column="city")

    print(f"\n–ù–∞–π–¥–µ–Ω–æ –≤ {len(results)} —Ñ–∞–π–ª–∞—Ö")
```

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

### –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã:

1. **Parquet - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç** –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö:
   - –°–∂–∞—Ç–∏–µ –≤ 10-20 —Ä–∞–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å CSV
   - –ë—ã—Å—Ç—Ä—ã–π –¥–æ—Å—Ç—É–ø –∫ –¥–∞–Ω–Ω—ã–º
   - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö

2. **–í–∞—à–∞ —Å–∏—Ç—É–∞—Ü–∏—è**:
   - 151 Parquet —Ñ–∞–π–ª –æ—Ç –ú–µ–≥–∞—Ñ–æ–Ω
   - –ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä: 9 –ì–ë
   - –ü–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ CSV: 63 –ì–ë
   - **–ü–æ—Ç–µ—Ä—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ 7 —Ä–∞–∑**

3. **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**:
   - ‚úÖ –ù–µ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å Parquet —Ñ–∞–π–ª—ã
   - ‚úÖ –†–∞–±–æ—Ç–∞—Ç—å —Å –Ω–∏–º–∏ —á–µ—Ä–µ–∑ pandas/spark
   - ‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ/—Å–≤–æ–¥–∫–∏ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞
   - ‚úÖ **–≠–∫–æ–Ω–æ–º–∏—è: 54 –ì–ë –¥–∏—Å–∫–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞**

### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:

1. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫–∏**:
   ```bash
   pip install pandas pyarrow duckdb
   ```

2. **–ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã** –∏–∑ —ç—Ç–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞

3. **–ò–∑–º–µ–Ω–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é** —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏:
   - –î–æ–±–∞–≤–∏—Ç—å `.parquet` –≤ —Å–ø–∏—Å–æ–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏

4. **–°–æ–∑–¥–∞—Ç—å –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Å–∫—Ä–∏–ø—Ç—ã** –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏

---

## –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [Apache Parquet Documentation](https://parquet.apache.org/docs/)
- [Pandas read_parquet](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html)
- [PyArrow Documentation](https://arrow.apache.org/docs/python/)
- [DuckDB SQL –¥–ª—è —Ñ–∞–π–ª–æ–≤](https://duckdb.org/)
- [Polars - –±—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑](https://www.pola.rs/)

---

**–î–æ–∫—É–º–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω**: 20 —è–Ω–≤–∞—Ä—è 2026
**–ê–≤—Ç–æ—Ä**: –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞ sync_ya_disk
**–í–µ—Ä—Å–∏—è**: 1.0
